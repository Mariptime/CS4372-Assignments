{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d033fbf",
   "metadata": {},
   "source": [
    " # Dog Breed Identification (MobileNetV2 transfer learning)\n",
    "\n",
    " I wrote this notebook to follow the assignment requirements:\n",
    "  - Keras / TensorFlow usage\n",
    "  - Transfer learning with MobileNetV2 (ImageNet weights)\n",
    "  - Fine-tuning capability\n",
    "  - No hard-coded local absolute paths (data is streamed from a public GCS URL)\n",
    "  - Stratified train/val/test split\n",
    "  - Experiment logging (parameters + metrics)\n",
    "  - History plots of accuracy & loss\n",
    "  - At least 25 test samples with Data, True Label, Predicted Label (CSV + montage image)\n",
    "\n",
    " **Important**: this notebook streams the ZIP from the public URL you provided:\n",
    " https://storage.googleapis.com/mariptime_assignment3_data/dog-breed-identification.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "badee74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime dir: d:\\Classes\\UTD\\Senior Year\\CS 4372.501 - Computational Methods for Data Scientists\\Assignments\\CS4372-Assignments\\Assignment 3\\runtime_data\n",
      "Output dir: d:\\Classes\\UTD\\Senior Year\\CS 4372.501 - Computational Methods for Data Scientists\\Assignments\\CS4372-Assignments\\Assignment 3\\results\n"
     ]
    }
   ],
   "source": [
    "# runtime / user parameters\n",
    "GCS_PUBLIC_ZIP = \"https://storage.googleapis.com/mariptime_assignment3_data/dog-breed-identification.zip\"\n",
    "# where to extract/run training (runtime-only, relative path)\n",
    "RUNTIME_DIR = \"runtime_data\"   # not an absolute path; created in current working directory\n",
    "OUTPUT_DIR = \"results\"         # artifacts (experiments.csv, plots, models, sample CSVs)\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "VAL_SIZE = 0.15\n",
    "TEST_SIZE = 0.10\n",
    "EPOCHS_HEAD = 6\n",
    "EPOCHS_FINETUNE = 4\n",
    "MIN_SAMPLE_PRED = 25\n",
    "\n",
    "# small hyperparameter grid (expand if you want more tuning)\n",
    "LR_LIST = [1e-3]               # head training LR\n",
    "DROPOUT_LIST = [0.3]\n",
    "UNFREEZE_LAST_N_LIST = [0, 50]  # 0 = no fine-tune; otherwise unfreeze last N layers of MobileNetV2\n",
    "\n",
    "# create directories\n",
    "import os\n",
    "os.makedirs(RUNTIME_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Runtime dir:\", os.path.abspath(RUNTIME_DIR))\n",
    "print(\"Output dir:\", os.path.abspath(OUTPUT_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2dc3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.1\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFont\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# seeds for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc2dfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12511173606684529828\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5711593472\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15035366103519077984\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12cc6000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "1 Physical GPUs, 1 Logical GPUs detected.\n"
     ]
    }
   ],
   "source": [
    "# === GPU setup ===\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# If a GPU is detected, allow memory growth (prevents TensorFlow from grabbing all VRAM at once)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs detected.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected — training will run on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9110d999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from: https://storage.googleapis.com/mariptime_assignment3_data/dog-breed-identification.zip\n",
      "Downloaded 724495926 bytes\n",
      "Extracted to runtime_data\n",
      "runtime_data\\labels.csv\n",
      "runtime_data\\sample_submission.csv\n",
      "runtime_data\\test\n",
      "runtime_data\\train\n"
     ]
    }
   ],
   "source": [
    "zip_url = GCS_PUBLIC_ZIP\n",
    "print(\"Downloading from:\", zip_url)\n",
    "\n",
    "r = requests.get(zip_url, stream=True)\n",
    "r.raise_for_status()\n",
    "\n",
    "zip_bytes = io.BytesIO()\n",
    "# stream write to BytesIO\n",
    "for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "    if chunk:\n",
    "        zip_bytes.write(chunk)\n",
    "zip_bytes.seek(0)\n",
    "print(\"Downloaded\", zip_bytes.getbuffer().nbytes, \"bytes\")\n",
    "\n",
    "# extract\n",
    "# with zipfile.ZipFile(zip_bytes) as zf:\n",
    "#     def safe_extract(zipf, extract_to):\n",
    "#         for member in zipf.namelist():\n",
    "#             # keep extraction inside extract_to\n",
    "#             member_path = os.path.normpath(os.path.join(extract_to, member))\n",
    "#             if not member_path.startswith(os.path.abspath(extract_to)):\n",
    "#                 raise Exception(\"Attempted path traversal in zip file\")\n",
    "#         zipf.extractall(extract_to)\n",
    "#     safe_extract(zf, RUNTIME_DIR)\n",
    "with zipfile.ZipFile(zip_bytes) as zf:\n",
    "    zf.extractall(RUNTIME_DIR)\n",
    "print(\"Extracted to\", RUNTIME_DIR)\n",
    "# show top-level contents\n",
    "for p in sorted(Path(RUNTIME_DIR).glob(\"*\"))[:20]:\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b2927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using labels CSV: runtime_data\\labels.csv\n",
      "Train images directory: runtime_data\\train\n"
     ]
    }
   ],
   "source": [
    "labels_csv_path = \"labels.csv\"\n",
    "print(\"Using labels CSV:\", labels_csv_path)\n",
    "\n",
    "train_images_dir = \"train\"\n",
    "print(\"Train images directory:\", train_images_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dec3480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total usable images: 10222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>runtime_data\\train\\000bec180eb18c7604dcecc8fe0...</td>\n",
       "      <td>boston_bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>runtime_data\\train\\001513dfcb2ffafc82cccf4d8bb...</td>\n",
       "      <td>dingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>runtime_data\\train\\001cdf01b096e06d78e9e5112d4...</td>\n",
       "      <td>pekinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>runtime_data\\train\\00214f311d5d2247d5dfe4fe24b...</td>\n",
       "      <td>bluetick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>runtime_data\\train\\0021f9ceb3235effd7fcde7f753...</td>\n",
       "      <td>golden_retriever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath             breed\n",
       "0  runtime_data\\train\\000bec180eb18c7604dcecc8fe0...       boston_bull\n",
       "1  runtime_data\\train\\001513dfcb2ffafc82cccf4d8bb...             dingo\n",
       "2  runtime_data\\train\\001cdf01b096e06d78e9e5112d4...          pekinese\n",
       "3  runtime_data\\train\\00214f311d5d2247d5dfe4fe24b...          bluetick\n",
       "4  runtime_data\\train\\0021f9ceb3235effd7fcde7f753...  golden_retriever"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = pd.read_csv(labels_csv_path)\n",
    "if 'id' not in labels_df.columns or 'breed' not in labels_df.columns:\n",
    "    raise ValueError(\"labels CSV must contain 'id' and 'breed' columns\")\n",
    "\n",
    "labels_df['filepath'] = labels_df['id'].astype(str) + '.jpg'\n",
    "labels_df['filepath'] = labels_df['filepath'].apply(lambda p: os.path.join(train_images_dir, p))\n",
    "\n",
    "# filter missing\n",
    "labels_df['exists'] = labels_df['filepath'].apply(os.path.exists)\n",
    "missing = (~labels_df['exists']).sum()\n",
    "if missing > 0:\n",
    "    print(f\"Warning: {missing} images referenced in CSV not found in images directory. They will be ignored.\")\n",
    "labels_df = labels_df[labels_df['exists']].reset_index(drop=True)\n",
    "labels_df = labels_df[['filepath','breed']]\n",
    "\n",
    "print(\"Total usable images:\", len(labels_df))\n",
    "labels_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f9a2e",
   "metadata": {},
   "source": [
    "## Stratified split into train / val / test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b42a4512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 7665 Val: 1534 Test: 1023\n",
      "Unique breeds: 120\n"
     ]
    }
   ],
   "source": [
    "def stratified_split(df, val_size=0.15, test_size=0.10, seed=42):\n",
    "    train_val_df, test_df = train_test_split(df, test_size=test_size, stratify=df['breed'], random_state=seed)\n",
    "    adjusted_val = val_size / (1.0 - test_size)\n",
    "    train_df, val_df = train_test_split(train_val_df, test_size=adjusted_val, stratify=train_val_df['breed'], random_state=seed)\n",
    "    return train_df.reset_index(drop=True), val_df.reset_index(drop=True), test_df.reset_index(drop=True)\n",
    "\n",
    "train_df, val_df, test_df = stratified_split(labels_df, val_size=VAL_SIZE, test_size=TEST_SIZE, seed=SEED)\n",
    "print(\"Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n",
    "print(\"Unique breeds:\", labels_df['breed'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdf4dc8",
   "metadata": {},
   "source": [
    "## Data generators (augmentation for training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52852810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7665 validated image filenames belonging to 120 classes.\n",
      "Found 1534 validated image filenames belonging to 120 classes.\n",
      "Found 1023 validated image filenames belonging to 120 classes.\n",
      "Number of classes: 120\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    zoom_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=(0.8, 1.2)\n",
    ")\n",
    "test_val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_gen = train_datagen.flow_from_dataframe(\n",
    "    train_df, x_col='filepath', y_col='breed',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical', shuffle=True, seed=SEED\n",
    ")\n",
    "val_gen = test_val_datagen.flow_from_dataframe(\n",
    "    val_df, x_col='filepath', y_col='breed',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical', shuffle=False\n",
    ")\n",
    "test_gen = test_val_datagen.flow_from_dataframe(\n",
    "    test_df, x_col='filepath', y_col='breed',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical', shuffle=False\n",
    ")\n",
    "\n",
    "class_indices = train_gen.class_indices\n",
    "idx_to_class = {v: k for k, v in class_indices.items()}\n",
    "print(\"Number of classes:\", len(class_indices))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a781658",
   "metadata": {},
   "source": [
    "## Utility: build MobileNetV2 transfer model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cee7705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes, img_size=224, dropout_rate=0.3, base_trainable=False, unfreeze_last_n=0):\n",
    "    input_shape = (img_size, img_size, 3)\n",
    "    base = MobileNetV2(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    base.trainable = base_trainable\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = base(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "\n",
    "    if unfreeze_last_n and unfreeze_last_n > 0:\n",
    "        base.trainable = True\n",
    "        total = len(base.layers)\n",
    "        freeze_up_to = max(0, total - unfreeze_last_n)\n",
    "        for i, layer in enumerate(base.layers):\n",
    "            layer.trainable = (i >= freeze_up_to)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26da87b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed class weights.\n"
     ]
    }
   ],
   "source": [
    "# compute class weights mapped to indices used by the generator\n",
    "breeds_sorted = sorted(class_indices.keys())\n",
    "y_train = train_df['breed'].values\n",
    "try:\n",
    "    cw_vals = compute_class_weight(class_weight='balanced', classes=np.array(breeds_sorted), y=y_train)\n",
    "    class_weight = {class_indices[breed]: float(w) for breed, w in zip(breeds_sorted, cw_vals)}\n",
    "    print(\"Computed class weights.\")\n",
    "except Exception as e:\n",
    "    print(\"Could not compute class weights:\", e)\n",
    "    class_weight = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232758ca",
   "metadata": {},
   "source": [
    "## Training loop: for each \n",
    "### hyperparameter combination, train head, optionally fine-tune, evaluate, and log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd8b708a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1: lr=0.001, dropout=0.3, unfreeze_last_n=0 ===\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               327936    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 120)               30840     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,616,760\n",
      "Trainable params: 358,776\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.96481, saving model to results\\best_model_run_1.h5\n",
      "240/240 - 248s - loss: 2.2203 - accuracy: 0.4664 - val_loss: 0.9648 - val_accuracy: 0.7066 - lr: 0.0010 - 248s/epoch - 1s/step\n",
      "Epoch 2/6\n",
      "\n",
      "Epoch 2: val_loss improved from 0.96481 to 0.85955, saving model to results\\best_model_run_1.h5\n",
      "240/240 - 75s - loss: 1.0268 - accuracy: 0.6960 - val_loss: 0.8595 - val_accuracy: 0.7282 - lr: 0.0010 - 75s/epoch - 312ms/step\n",
      "Epoch 3/6\n",
      "\n",
      "Epoch 3: val_loss improved from 0.85955 to 0.83434, saving model to results\\best_model_run_1.h5\n",
      "240/240 - 75s - loss: 0.8730 - accuracy: 0.7401 - val_loss: 0.8343 - val_accuracy: 0.7334 - lr: 0.0010 - 75s/epoch - 311ms/step\n",
      "Epoch 4/6\n",
      "\n",
      "Epoch 4: val_loss improved from 0.83434 to 0.80221, saving model to results\\best_model_run_1.h5\n",
      "240/240 - 75s - loss: 0.7738 - accuracy: 0.7605 - val_loss: 0.8022 - val_accuracy: 0.7477 - lr: 0.0010 - 75s/epoch - 314ms/step\n",
      "Epoch 5/6\n",
      "\n",
      "Epoch 5: val_loss improved from 0.80221 to 0.75132, saving model to results\\best_model_run_1.h5\n",
      "240/240 - 75s - loss: 0.6996 - accuracy: 0.7796 - val_loss: 0.7513 - val_accuracy: 0.7503 - lr: 0.0010 - 75s/epoch - 314ms/step\n",
      "Epoch 6/6\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.75132\n",
      "240/240 - 83s - loss: 0.6451 - accuracy: 0.7967 - val_loss: 0.8105 - val_accuracy: 0.7575 - lr: 0.0010 - 83s/epoch - 347ms/step\n",
      "Head trained in 632.5s\n",
      "After head: val_loss=0.8105, val_acc=0.7575\n",
      "Predicting on test set...\n",
      "32/32 [==============================] - 20s 637ms/step\n",
      "Test acc: 0.7781, logloss: 0.7353\n",
      "Run 1 complete: test_acc=0.7781\n",
      "\n",
      "=== Run 2: lr=0.001, dropout=0.3, unfreeze_last_n=50 ===\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               327936    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 120)               30840     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,616,760\n",
      "Trainable params: 358,776\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.92828, saving model to results\\best_model_run_2.h5\n",
      "240/240 - 77s - loss: 2.2162 - accuracy: 0.4720 - val_loss: 0.9283 - val_accuracy: 0.7249 - lr: 0.0010 - 77s/epoch - 320ms/step\n",
      "Epoch 2/6\n",
      "\n",
      "Epoch 2: val_loss improved from 0.92828 to 0.88907, saving model to results\\best_model_run_2.h5\n",
      "240/240 - 75s - loss: 1.0424 - accuracy: 0.6905 - val_loss: 0.8891 - val_accuracy: 0.7262 - lr: 0.0010 - 75s/epoch - 312ms/step\n",
      "Epoch 3/6\n",
      "\n",
      "Epoch 3: val_loss improved from 0.88907 to 0.82272, saving model to results\\best_model_run_2.h5\n",
      "240/240 - 72s - loss: 0.8682 - accuracy: 0.7367 - val_loss: 0.8227 - val_accuracy: 0.7477 - lr: 0.0010 - 72s/epoch - 300ms/step\n",
      "Epoch 4/6\n",
      "\n",
      "Epoch 4: val_loss improved from 0.82272 to 0.81375, saving model to results\\best_model_run_2.h5\n",
      "240/240 - 76s - loss: 0.7770 - accuracy: 0.7546 - val_loss: 0.8138 - val_accuracy: 0.7510 - lr: 0.0010 - 76s/epoch - 318ms/step\n",
      "Epoch 5/6\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.81375\n",
      "240/240 - 76s - loss: 0.7048 - accuracy: 0.7812 - val_loss: 0.8293 - val_accuracy: 0.7438 - lr: 0.0010 - 76s/epoch - 315ms/step\n",
      "Epoch 6/6\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.81375\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "240/240 - 75s - loss: 0.6468 - accuracy: 0.7941 - val_loss: 0.8271 - val_accuracy: 0.7386 - lr: 0.0010 - 75s/epoch - 312ms/step\n",
      "Head trained in 451.4s\n",
      "After head: val_loss=0.8271, val_acc=0.7386\n",
      "Starting fine-tuning with last 50 layers unfrozen.\n",
      "Epoch 1/4\n",
      "\n",
      "Epoch 1: val_loss improved from 0.81375 to 0.70266, saving model to results\\best_model_run_2.h5\n",
      "240/240 - 79s - loss: 0.4832 - accuracy: 0.8384 - val_loss: 0.7027 - val_accuracy: 0.7797 - lr: 1.0000e-04 - 79s/epoch - 329ms/step\n",
      "Epoch 2/4\n",
      "\n",
      "Epoch 2: val_loss improved from 0.70266 to 0.69661, saving model to results\\best_model_run_2.h5\n",
      "240/240 - 75s - loss: 0.4460 - accuracy: 0.8561 - val_loss: 0.6966 - val_accuracy: 0.7797 - lr: 1.0000e-04 - 75s/epoch - 314ms/step\n",
      "Epoch 3/4\n",
      "\n",
      "Epoch 3: val_loss improved from 0.69661 to 0.68924, saving model to results\\best_model_run_2.h5\n",
      "240/240 - 76s - loss: 0.4238 - accuracy: 0.8660 - val_loss: 0.6892 - val_accuracy: 0.7816 - lr: 1.0000e-04 - 76s/epoch - 316ms/step\n",
      "Epoch 4/4\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.68924\n",
      "240/240 - 75s - loss: 0.4217 - accuracy: 0.8647 - val_loss: 0.6965 - val_accuracy: 0.7797 - lr: 1.0000e-04 - 75s/epoch - 313ms/step\n",
      "Fine-tune done in 305.8s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# load best weights if checkpoint exists\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(ckpt):\n\u001b[1;32m---> 62\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# evaluate on test set\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicting on test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aksha\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\aksha\\miniconda3\\envs\\tf\\lib\\site-packages\\numpy\\core\\fromnumeric.py:655\u001b[0m, in \u001b[0;36mtranspose\u001b[1;34m(a, axes)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_transpose_dispatcher)\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranspose\u001b[39m(a, axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    590\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;124;03m    Returns an array with axes transposed.\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    653\u001b[0m \n\u001b[0;32m    654\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtranspose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aksha\\miniconda3\\envs\\tf\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "experiment_rows = []\n",
    "run_id = 0\n",
    "\n",
    "for lr in LR_LIST:\n",
    "    for dropout in DROPOUT_LIST:\n",
    "        for unfreeze_last_n in UNFREEZE_LAST_N_LIST:\n",
    "            run_id += 1\n",
    "            print(f\"\\n=== Run {run_id}: lr={lr}, dropout={dropout}, unfreeze_last_n={unfreeze_last_n} ===\")\n",
    "            tf.keras.backend.clear_session()\n",
    "            model = build_model(num_classes=len(class_indices), img_size=IMG_SIZE, dropout_rate=dropout, base_trainable=False, unfreeze_last_n=0)\n",
    "            model.compile(optimizer=optimizers.Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "            model.summary()\n",
    "            ckpt = os.path.join(OUTPUT_DIR, f\"best_model_run_{run_id}.h5\")\n",
    "            cbs = [\n",
    "                ModelCheckpoint(ckpt, save_best_only=True, monitor='val_loss', verbose=1),\n",
    "                EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
    "            ]\n",
    "            t0 = time.time()\n",
    "            hist_head = model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS_HEAD, callbacks=cbs, class_weight=class_weight, verbose=2)\n",
    "            t_head = time.time() - t0\n",
    "            print(f\"Head trained in {t_head:.1f}s\")\n",
    "\n",
    "            val_loss, val_acc = model.evaluate(val_gen, verbose=0)[0:2]\n",
    "            print(f\"After head: val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "            hist_finetune = None\n",
    "            if unfreeze_last_n and unfreeze_last_n > 0:\n",
    "                # set trainability on base layers\n",
    "                base_layer = None\n",
    "                # locate base MobileNetV2 inside model by finding a layer with many children (has attribute 'layers')\n",
    "                for layer in model.layers:\n",
    "                    if hasattr(layer, 'layers') and len(getattr(layer, 'layers')) > 0:\n",
    "                        # check name pattern\n",
    "                        if 'mobilenet' in layer.name.lower() or 'conv' in layer.name.lower() or 'block' in layer.name.lower():\n",
    "                            base_layer = layer\n",
    "                            break\n",
    "                # fallback: assume model.layers[1] is base\n",
    "                if base_layer is None and len(model.layers) > 1:\n",
    "                    base_layer = model.layers[1]\n",
    "                if base_layer is not None and hasattr(base_layer, 'layers'):\n",
    "                    total = len(base_layer.layers)\n",
    "                    freeze_up_to = max(0, total - unfreeze_last_n)\n",
    "                    for i, bl in enumerate(base_layer.layers):\n",
    "                        bl.trainable = (i >= freeze_up_to)\n",
    "                else:\n",
    "                    # fallback: set all layers trainable then re-freeze initial portion by name\n",
    "                    for layer in model.layers:\n",
    "                        layer.trainable = True\n",
    "                # recompile with lower lr for fine-tuning\n",
    "                model.compile(optimizer=optimizers.Adam(learning_rate=lr*0.1), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                print(\"Starting fine-tuning with last\", unfreeze_last_n, \"layers unfrozen.\")\n",
    "                t1 = time.time()\n",
    "                hist_finetune = model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS_FINETUNE, callbacks=cbs, class_weight=class_weight, verbose=2)\n",
    "                t_finetune = time.time() - t1\n",
    "                print(f\"Fine-tune done in {t_finetune:.1f}s\")\n",
    "\n",
    "            # load best weights if checkpoint exists\n",
    "            if os.path.exists(ckpt):\n",
    "                model.load_weights(ckpt)\n",
    "\n",
    "            # evaluate on test set\n",
    "            print(\"Predicting on test set...\")\n",
    "            proba = model.predict(test_gen, verbose=1)\n",
    "            preds = np.argmax(proba, axis=1)\n",
    "            true_inds = test_gen.classes  # integer indices consistent with train_gen.class_indices\n",
    "            test_acc = accuracy_score(true_inds, preds)\n",
    "            try:\n",
    "                test_logloss = log_loss(tf.keras.utils.to_categorical(true_inds, num_classes=len(class_indices)), proba)\n",
    "            except Exception:\n",
    "                test_logloss = float('nan')\n",
    "            print(f\"Test acc: {test_acc:.4f}, logloss: {test_logloss:.4f}\")\n",
    "\n",
    "            # save history plot (use finetune history if present, else head history)\n",
    "            def plot_history(hist, out_path):\n",
    "                plt.figure(figsize=(10,4))\n",
    "                plt.subplot(1,2,1)\n",
    "                plt.plot(hist.history.get('accuracy', []), label='train_acc')\n",
    "                plt.plot(hist.history.get('val_accuracy', []), label='val_acc')\n",
    "                plt.title('Accuracy')\n",
    "                plt.legend()\n",
    "                plt.subplot(1,2,2)\n",
    "                plt.plot(hist.history.get('loss', []), label='train_loss')\n",
    "                plt.plot(hist.history.get('val_loss', []), label='val_loss')\n",
    "                plt.title('Loss')\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(out_path)\n",
    "                plt.close()\n",
    "\n",
    "            hist_plot = os.path.join(OUTPUT_DIR, f\"history_run_{run_id}.png\")\n",
    "            if hist_finetune:\n",
    "                plot_history(hist_finetune, hist_plot)\n",
    "            else:\n",
    "                plot_history(hist_head, hist_plot)\n",
    "\n",
    "            # sample at least MIN_SAMPLE_PRED rows from test set evenly\n",
    "            n_sample = max(MIN_SAMPLE_PRED, min(len(test_df), MIN_SAMPLE_PRED))\n",
    "            sample_idxs = np.linspace(0, len(test_df)-1, n_sample, dtype=int)\n",
    "            sample_rows = []\n",
    "            for idx in sample_idxs:\n",
    "                fp = test_df.iloc[idx]['filepath']\n",
    "                true_label = test_df.iloc[idx]['breed']\n",
    "                pred_label = idx_to_class[preds[idx]]\n",
    "                sample_rows.append({'filepath': fp, 'true_label': true_label, 'predicted_label': pred_label})\n",
    "            sample_df = pd.DataFrame(sample_rows)\n",
    "            sample_csv = os.path.join(OUTPUT_DIR, f\"sample_predictions_run_{run_id}.csv\")\n",
    "            sample_df.to_csv(sample_csv, index=False)\n",
    "\n",
    "            # create montage image of sample predictions\n",
    "            def make_montage(df_rows, out_file, thumb=IMG_SIZE):\n",
    "                cols = 5\n",
    "                rows_n = math.ceil(len(df_rows)/cols)\n",
    "                montage = Image.new('RGB', (cols*thumb, rows_n*thumb), color=(255,255,255))\n",
    "                draw = ImageDraw.Draw(montage)\n",
    "                for i, r in enumerate(df_rows):\n",
    "                    try:\n",
    "                        im = Image.open(r['filepath']).convert('RGB')\n",
    "                        im = ImageOps.fit(im, (thumb, thumb), Image.LANCZOS)\n",
    "                    except Exception:\n",
    "                        im = Image.new('RGB', (thumb, thumb), (200,200,200))\n",
    "                    x = (i % cols) * thumb\n",
    "                    y = (i // cols) * thumb\n",
    "                    montage.paste(im, (x,y))\n",
    "                    # write text: True / Pred (use small rectangle for legibility)\n",
    "                    text = f\"T:{r['true_label']}\\nP:{r['predicted_label']}\"\n",
    "                    draw.rectangle([x+3, y+3, x+thumb-3, y+30], fill=(0,0,0,120))\n",
    "                    draw.text((x+6, y+4), text, fill=(255,255,255))\n",
    "                montage.save(out_file)\n",
    "\n",
    "            montage_path = os.path.join(OUTPUT_DIR, f\"sample_predictions_run_{run_id}.png\")\n",
    "            make_montage(sample_rows, montage_path)\n",
    "\n",
    "            # log experiment\n",
    "            experiment_rows.append({\n",
    "                'run_id': run_id,\n",
    "                'lr': lr,\n",
    "                'dropout': dropout,\n",
    "                'unfreeze_last_n': unfreeze_last_n,\n",
    "                'val_loss': float(val_loss),\n",
    "                'val_acc': float(val_acc),\n",
    "                'test_loss': float(test_logloss),\n",
    "                'test_acc': float(test_acc),\n",
    "                'ckpt_path': ckpt,\n",
    "                'history_plot': hist_plot,\n",
    "                'sample_csv': sample_csv,\n",
    "                'sample_montage': montage_path\n",
    "            })\n",
    "\n",
    "            print(f\"Run {run_id} complete: test_acc={test_acc:.4f}\")\n",
    "# write experiments CSV\n",
    "experiments_df = pd.DataFrame(experiment_rows)\n",
    "experiments_csv = os.path.join(OUTPUT_DIR, \"experiments.csv\")\n",
    "experiments_df.to_csv(experiments_csv, index=False)\n",
    "print(\"Experiments saved to\", experiments_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc75112",
   "metadata": {},
   "source": [
    "## Show experiments table (top runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d75895",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_df = pd.read_csv(os.path.join(OUTPUT_DIR, \"experiments.csv\"))\n",
    "experiments_df.sort_values('test_acc', ascending=False, inplace=True)\n",
    "experiments_df.reset_index(drop=True, inplace=True)\n",
    "experiments_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8723c5c2",
   "metadata": {},
   "source": [
    "## Display sample predictions from best run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d92c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(experiments_df) == 0:\n",
    "    print(\"No experiments found.\")\n",
    "else:\n",
    "    best = experiments_df.iloc[0]\n",
    "    print(\"Best run:\", best.to_dict())\n",
    "    sample_csv = best['sample_csv']\n",
    "    sample_img = best['sample_montage']\n",
    "    display_df = pd.read_csv(sample_csv)\n",
    "    display(display_df.head(50))\n",
    "    # show montage\n",
    "    from IPython.display import Image, display as idisp\n",
    "    idisp(Image(filename=sample_img, width=900))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
